[TOC]

### Machine Learning with Adversaries: Byzantine Tolerant Gradient Descent

https://dl.acm.org/doi/pdf/10.5555/3294771.3294783

首先证明，基于worker提出的向量线性组合的梯度聚合规则（即当前方法）不能容忍单一拜占庭失败。

- 单个拜占庭节点就可以强迫一个参数服务器选择任意一个向量
- 一种基于平方距离的非线性聚合规则也许可以，但是也只能容忍一个拜占庭节点，两个拜占庭工人可以互相勾结。

然后==制定了聚合规则的弹性属性==，捕获了在f个拜占庭节点存在情况下保证收敛的基本要求。

- 结合基于多数和基于平方距离的方法的直觉，我们可以选择最接近其n− f 邻居的向量，即使得到其n-f最近向量的距离平方和最小的向量——Krum背后的主要思想。

最后==提出了Krum==，这是一个满足我们的弹性属性的聚合规则，我们认为这是第一个用于分布式SGD的可证明拜占庭弹性算法。我们还报告了Krum的实验评估。

#### 1 Introduction

分布式系统中屏蔽故障的一种经典方法是使用状态机复制协议[26]，但它要求所有工作进程都应用状态转换。在分布式机器学习的情况下，这种约束可以通过两种方式转换，但都不令人满意。

- 过程在更新其局部参数向量的数据样本上达成一致
  - 必须将数据样本传输到每个进程，然后该进程必须执行重量级计算以更新其局部参数向量。这需要通信和计算成本，无法实现分配工作的整个目的。
- 在更新参数向量的方式上达成一致。
  - 过程无法检查所选的参数向量更新是否已在真实数据上正确计算。拜占庭过程可能提出了更新，并可能很容易阻止学习算法的收敛。

**本文解决了一个基本问题：如何设计一个分布式SGD来容忍n个节点之间的f个拜占庭过程。**

我们还评估了Multi-Krum，它是Krum的一种变体，直观地在Krum和平均值之间插值，从而允许混合Krum弹性特性和平均值的收敛速度。

#### 3 Byzantine Resilience

新概念$(\alpha,f)$-拜占庭恢复性

<img src="C:\Users\xxl\AppData\Roaming\Typora\typora-user-images\image-20220805173113847.png" alt="image-20220805173113847" style="zoom: 33%;" />

#### 4 The Krum Function

基于平方距离的重心聚合规则不具有拜占庭弹性。这样的求和涉及所有向量，甚至是那些非常远的向量。通过提出足够大的向量，拜占庭节点可以迫使总重心更接近另一个拜占庭节点提出的向量。避免这样的问题的方法是排除距离太远的向量。

Krum的定义：对于每个节点，定义一个分数，是距离该节点最近的n-f-2个节点的距离和，取距离和最小的节点的参数作为聚集参数。

Krum是$(\alpha,f)$-拜占庭弹性的，时间复杂度为$O(n^2d)$，其中d是维度。

#### 7 Concluding Remarks

分布式计算视角：即使看似相关，d维近似一致的结果不能应用于拜占庭弹性机器环境，原因如下：

- [24,14] 假设可以向协议实例提出的向量集是有界的，因此至少f+1个正确的工作者提出相同的向量，这将需要在我们的设置中进行大量冗余工作
- [24] 要求$O(n^d)$中的每个工作者进行局部计算。如果维度很大，成本会过大，考虑复杂度时，d是一个关键参数。

统计和机器学习视角：使用了经典理论统计中的几何中值和分解概念的稳定性。

- 本文工作最接近分解概念的是可容忍拜占庭节点的最大比例 (n-2)/2n，在n上渐近地达到最佳理论值1/2。
- 已知几何中值确实达到最佳分解。然而，目前还不知道计算几何中值的闭合形式或精确算法。几何中值一个易于计算的版本是Medoid，提出的向量最小化其他提出向量的距离之和。Medoid可以用一个类似Krum的算法计算，但是multi-krum的实现优于Medoid。

模型内的鲁棒性：本文从一个粗粒度的角度解决了鲁棒性，故障单元是一个节点，根据本地数据或来自服务器的委托数据接收其模型副本并估计梯度。

- 需要考虑的一个自然问题是细粒度视图：模型本身是否对内部扰动具有鲁棒性？这种细粒度视图的许多实际结果[9]之一是理解深度学习中的内存成本降低权衡。



### Byzantine-Robust Distributed Learning: Towards Optimal Statistical Rates

http://proceedings.mlr.press/v80/yin18a/yin18a.pdf

对于强凸问题，不管通信成本如何，没有任何算法可以实现低于 $\tilde{\Omega}(\dfrac{\alpha}{\sqrt{n}} + \dfrac{1}{\sqrt{nm}}) = \tilde{\Omega}(\dfrac{1}{\sqrt{n}}(\alpha + \dfrac{1}{\sqrt{m}}))$ 的误差。直观地说，这个错误率是我们应该作为目标的最佳错误率。因为

- $1/\sqrt{n}$ 是具有n个数据点的每台机器的有效标准偏差
- $\alpha$ 是拜占庭机器的偏差影响
- $1/\sqrt{m}$ 是m个正常机器的平均影响

- 当没有或有很少的拜占庭机器时，我们会看到通常的缩放 $1/\sqrt{mn}$ 表示数据点的总数；当一些机器是拜占庭式的时，它们的影响仍然是有界的，而且与 $\alpha$ 成正比。
- 如果一个算法保证达到这个界限，我们就可以放心，在试图防范拜占庭失败时，我们不会牺牲学习质量。我们付出了不可避免的代价，但在拜占庭失败的情况下，我们会获得尽可能高的统计精度。

另一个重要的考虑因素是沟通效率。理想的算法应该包括少量的通信循环，以及每一循环传输的少量数据。我们考虑这样一种设置，即在每一轮中，工作机器或主机器只能传送大小为 $O(d)$ 的向量，其中d是要学习的参数的维数。在这种情况下，总通信成本与通信轮数成比例。



### The Hidden Vulnerability of Distributed Learning in Byzantium

https://arxiv.org/pdf/1802.07927.pdf

本文贡献：构建一个简单攻击，实验性地展示了它对CIFAR-10和MNIST的强效果。==引入Bulyan==，并且证明它显著地将攻击者的回旋余地减少到了一个狭窄的$O(1/\sqrt{d})$界限，经验证明Bulyan没有遭受现有聚合规则的脆弱性，并且在所需batch size中以合理的成本实现了收敛，就好像只有非拜占庭梯度用于更新模型一样。

#### 1 Introduction

鲁棒ML主要针对三种攻击：

- 投毒攻击：敌人在训练阶段注射有毒数据
- 探索攻击：一个好奇攻击者试图推断隐私敏感的信息
- 规避攻击：攻击者试图利用恶意输入愚弄一个已经训练好的模型

三种攻击是互补的，每种攻击都是一种挑战。

SGD即使在有对手存在的情况下也可以收敛，一般的方法是在对手存在的情况下找到梯度的稳健估计量，并倾向于该估计量，而不是所提供梯度的线性组合（因为其不稳健）。

#### 2 Model

2.3 梯度聚集规则（GAR）

聚集规则使用线性组合的缺点：敌人可以完全掌控聚集梯度。

- Brute：选中n-f个最聚集的梯度，并且将他们的平均值作为最终输出
- Krum：选中与其他向量最近的向量
- Geometric Median：真实值可能面临计算问题，但是可以近似得到。
  - 利用较易计算的 Medoid：距离和的提议向量中的任意最小值。由于有很多最小值，可以将GeoMed认为是有最小索引的提议向量的Medoid

#### 4 Bulyan

除了在确保收敛的意义上具有拜占庭弹性外，我们的算法Bulyan还确保每个坐标在由拜占庭-弹性聚合规则 A 选择的大多数向量上达成一致。该规则 A 可以是任何基于$l^p$范数或无穷范数的拜占庭弹性规则例如Brute、Krum、Medoid、GeoMed。

#### 6 Concluding remarks

拜占庭故障模型被认为只针对分布式计算的，但其实也适用于单机的投毒攻击，以及在集中式单节点环境中使用不可靠数据进行学习。

==寻找非凸成本函数可能的“最佳方向”问题仍然是机器学习优化中最具挑战性的问题之一，特别是当我们坚持使用SGD这样一种简单的一阶方法，并避免如（Reddi等人，2017）所指出的昂贵的类Hessian计算时。==

文章提出的方法解决了训练时发生的问题，目的是避免投毒攻击导致的不良模型。但是没有解决逃避攻击的问题，即：使用对抗性输入攻击已经训练过的模型。

从分布式计算的角度来看，似乎有两种关于鲁棒性的互补观点：

（1）粗粒度鲁棒性（梯度聚合方案的质量）

- 故障单元是单个机器，承载模型的副本（或模型的重要部分），并在试图估计梯度时受到攻击。

和（2）细粒度鲁棒性（模型的质量，与单参数相关）。

- ML使用的模型（例如神经网络）本身可以被视为分布式计算对象（Piuri，2001），其中故障单元是单个神经元和权重值。

与dropout算法有意外的联系——最初用于容错而后作为稳健的正则化方法

最近，从分布式计算的角度来看，稳健性与从学习性能的角度来看的稳健性之间的这种差距正在重现，问题包括：如果模型的单元错误，会向前传播多少错误？

==文章认为，应该从细粒度和粗粒度的角度研究投毒攻击。==例如，维数灾难攻击（第3节）是由细粒度（单参数）攻击引起的粗粒度问题（整个系统在模型上是错误的）的一个示例。因此，可以提出一个有趣的问题：给定单个权重值中的误差对模型输出的影响程度的界限，我们可以计算梯度的单个分量的投毒攻击的影响吗？随后，是否可以利用这一联系来发现任何投毒防御的理论极限？



### Generalized Byzantine-tolerant SGD

https://arxiv.org/pdf/1802.10116.pdf

#### 1 Introduction

文章考虑最常见的故障模型：拜占庭故障，攻击者可以知道其他进程的任何信息，并攻击传输中的任何值。更具体地说，机器之间的数据传输可以被任何值取代。在这种模型下，故障或攻击者没有任何约束。

本文研究**同步SGD**的拜占庭恢复性，是PS架构下流行的学习算法。总是等待从所有工作节点中收集梯度，然后进行下一次迭代。

对于经典拜占庭模型，所有拜占庭值必须位于相同的worker（行）中，而对于广义拜占庭模型，则没有此类约束。先前的工作讨论的是经典模型，本文泛化了模型。

有多种可能的攻击种类。通常情况下，攻击者想要扰乱模型训练，即使得SGD收敛慢或者收敛到一个坏结果。列举可能的攻击如下：

- gamber（最常见的攻击）：攻击者可以更改通信介质（如导线或网络接口）上的部分数据。攻击者随机选取数据并恶意更改它们（例如，将其乘以一个较大的负值）。因此，在服务器节点上，收集的梯度部分替换为任意值。
- omniscient：攻击者被认为知道所有worker发送的梯度，并且使用所有梯度的之和（按较大负值缩放）来替换一些梯度向量。其目的是误导SGD以更大的步幅进入相反的方向。
- Gaussian attack（较弱的攻击）：一些梯度向量被从方差较大的高斯分布中采样的随机向量替换。此类攻击者不需要worker提供任何信息。

我们提出了新的基于中值的聚合规则，其中SGD在一定条件下具有拜占庭弹性：对于每个维度，在由n个worker提供的所有n个值中，拜占庭值的数量必须小于n的一半。这种拜占庭弹性性质被称为“维度拜占庭弹性”。本文的主要贡献如下：

- 为同步SGD提出三种聚合规则：geometric median，marginal median，“mean around median”。==首个从理论和实证上研究了非凸环境下基于中值的聚合规则的论文。==
- 证明提出的三种方法有低计算开销，时间复杂度几乎是线性的，与非拜占庭聚合的默认选择平均相同。
- 制定了维度拜占庭弹性性质，并证明marginal median和“mean around median”是维度拜占庭弹性。==首个研究同步SGD的广义拜占庭故障和维度拜占庭弹性的论文。==

#### 3 Byzantine Resilience

经典拜占庭弹性性质的广义版本：维度拜占庭弹性（dimensional Byzantine Resilience）

- 对于每个维度，在由n个worker提供的所有n个值中，拜占庭值的数量必须小于n的一半

服务器节点并不知道哪些worker是拜占庭的，只知道数量。

#### 4 Median-based Aggregation

- Geometric Median

  <img src="C:\Users\xxl\AppData\Roaming\Typora\typora-user-images\image-20220815142120954.png" alt="image-20220815142120954" style="zoom: 50%;" />

- Marginal Median

  <img src="C:\Users\xxl\AppData\Roaming\Typora\typora-user-images\image-20220815142204453.png" alt="image-20220815142204453" style="zoom:50%;" />

- Beyond Median

  <img src="C:\Users\xxl\AppData\Roaming\Typora\typora-user-images\image-20220815143328502.png" alt="image-20220815143328502" style="zoom:50%;" />

#### 5 Experiment

mean不是拜占庭可恢复性的，medoid（计算高效的GeoMed）不是拜占庭可恢复的，但是可以抵御轻微的攻击（如高斯攻击）

Krum, MultiKrum, and GeoMed是经典拜占庭可恢复性，但不是维度拜占庭可恢复性。

MarMed and MeaMed 是维度拜占庭可恢复性的。

MeaMed的表现在所有的情况下都是最好的，MultiKrum也不错，只是不是维度拜占庭可恢复的。

MeaMed和Multi Krum具有更好性能的原因是它们利用了拜占庭值数量的额外信息。

MarMed有最低的计算成本。最糟糕的情况是全知攻击，但其在现实中很难实现。因此，对于大多数应用程序，我们建议MarMed作为一种易于实现的具有强大性能的聚合规则，且不需要知道拜占庭值的数量（重要的）。

#### 6 Related Works

本文方法的优势：

- 比起Krum更低的计算成本：GeoMed有近乎线性的时间复杂度，MarMed和MeaMed有线性复杂度$O(nd)$，而Krum是$O(n^2d)$
- 需要更少的先验知识：Krum需要知道拜占庭节点的数量q来计算n-q-2个最近邻居的欧氏距离。除此之外，当q已知或很好地估计时，MeaMed展现了比Krum更好的鲁棒性
- 维度拜占庭恢复性：MarMed和MeaMed
- 对多服务器节点更好地支持：如果整个参数集被分割并存储在多个服务器节点上，则MarMed和MeaMed不需要额外的通信，而Krum和GeoMed需要服务器节点之间的通信。



### A Little Is Enough: Circumventing Defenses For Distributed Learning

https://arxiv.org/pdf/1902.06156v1.pdf

攻击方法不仅可以阻止收敛，也可以重构模型行为（引入后门）。说明了==小部分特定的变化就可以实现一个不被任何防御所侦测的攻击。==

后门攻击：敌人可以修改个人特征或原始训练数据集的小区域来将后门嵌入模型，如果输入包含后门特征，模型就会根据敌人目标表现。

- 后门可以是单个样本，例如错误地将一个特定的人分类为另一个人，也可以是一类样本，例如在图像中设置特定的像素模式将导致它被恶意分类。

#### 1 Introduction

网络结构（层数、种类、大小）是提前在所有节点中达成共识的。

byzantine worker是训练过程中任意作恶的节点

- 在联合学习应用程序中，许多设备可能非常不可靠，甚至很容易被黑客破坏。 我们称这些设备为**Byzantine workers**。

#### 2 Background

作恶目标：阻止收敛、引入后门（数据投毒）

现有的攻击：后门攻击

现有的防御机制：

- Trimmed Mean：对于第 j 个参数，master 对 m 个 worker 的第 j 个参数进行排序，删除最大和最小的 $\beta$ 个参数，计算其余$m-2\beta$ 个参数的平均值作为全局模型的第 j 个参数
- Krum：m个worker中选一个与其他worker参数最为接近的worker（欧氏距离），将该worker的参数作为全局参数
  - 方法的本质是在正态分布中，每个维度中具有平均参数的向量将与来自同一分布的所有参数向量最接近。
  - Krum 的想法是即使所选择的局部模型来自受破坏的工作节点设备，其能够产生的影响也会受到限制，因为它与其他可能的正常工作节点设备的局部模型相似。
- Bulyan（state-of-art）：结合了前两者方法

#### 3 Our Attack

1. 找到一个扰动范围：参数不会被Trimmed Mean发现

   - 寻求一个可以偏离平均值而不被发现的范围。由于正态分布是对称的，相同的zmax值将为平均值周围的适用变化设置上下限。
   - 使得非作恶节点的值远离平均值，使得所有的作恶节点的值以高概率在一个以均值为中心的范围内

2. 克服Krum和Bulyan防御

   - 实际操作中，即使是很高维的空间，最好的节点也有几个参数是远离均值的
   - 利用这一缺点，生成一个参数集使得每个参数都只远离均值一点，这些微小的改变降低了欧氏距离，使得该恶意集被选中
   - 相比TrimmedMean，攻击Krum的方法的优势是仅需要很少的作恶节点进行均值和标准差的估计，并且只有一个节点需要报告作恶参数

3. 阻止收敛

   - 使用算法3

     <img src="C:\Users\xxl\AppData\Roaming\Typora\typora-user-images\image-20220801214455939.png" alt="image-20220801214455939" style="zoom: 33%;" />

4. 后门攻击

   - 攻击者在扰动范围内寻找一组参数，这些参数将为后门生成所需的标签，同时将对良性输入功能的影响降至最低。为了实现这一点，与（Bagdasaryan等人，2018）类似，攻击者将利用后门优化模型，同时最小化与原始参数的距离。

     <img src="C:\Users\xxl\AppData\Roaming\Typora\typora-user-images\image-20220801221511812.png" alt="image-20220801221511812" style="zoom: 33%;" />

总结：本文针对Krum和Bulyan防御实施攻击，证明微小的改变可以造成大攻击



### WMDefense: Using Watermark to Defense Byzantine Attacks in Federated Learning

https://ieeexplore.ieee.org/document/9798217/

通过在全局模型中嵌入水印并在局部模型训练后跟踪水印衰退程度来识别恶意客户端。

#### 1 Introduction

大多数现有的拜占庭鲁棒FL算法没有设计合适的恶意客户端检测机制。此外，它会导致恶意客户端作为良性客户端包含在全局模型聚合过程中，并对全局模型造成无法计算的影响。

文章贡献：

- 提出了一种新的拜占庭鲁棒FL算法WMDefense，可以通过快速多重检测识别所有恶意客户端，并且对恶意客户端的比例没有限制。因此，它可以帮助全局模型快速摆脱恶意更新。
- ==首次将模型水印技术引入拜占庭鲁棒FL算法。==具体而言，利用深度神经网络水印技术对全局模型进行水印，并通过检测模型水印受到的影响程度来识别恶意客户端。
- 通过对MNIST和CIFAR-10数据集的最新拜占庭攻击来评估WMD防御。此外，实验结果表明，可以检测到所有恶意客户端并抵御这些攻击。

#### 2 Related Work

最先进的拜占庭防御方法：FLTrust——使用受信任root数据集生成的模型更新来指导所有客户端的模型更新。在FLTrust中，服务器收集一个小的干净的训练数据集用于模型训练，服务提供商基于该数据集训练服务器模型。FLTrust通过比较本地模型更新的方向和服务器模型更新的偏差来处理恶意的本地模型更新。

水印的想法是将水印模型放到只有防御者知道的异常输入输出对。这可用于声明模型的所有权。这些异常值通常是通过向输入插入特殊触发器（例如，图像的非侵入性位置中的小正方形）来创建的。出于这个原因，水印可以被认为是一种投毒，尤其是后门插入[17]，由防御者永久使用。

#### 3 Definition

攻击模型

- Min-Max attack：攻击者可以任意操纵恶意客户端发送给服务器的本地模型。具体而言，攻击者使用已知的一些良性数据样本计算良性reference aggregate；然后，它通过在恶意方向上最大程度地扰动良性reference aggregate来计算恶意模型更新。攻击者计算恶意梯度，使其与任何其他梯度的最大距离以任何两个良性梯度之间的最大距离为上限。
- LIE attack：不需要聚合规则的知识，并且利用了客户梯度之间的经验差异。它给良性客户端的梯度更新添加了一些噪声，从而毒害了全局模型。由于它只添加少量噪声，因此在检测过程中对方差没有太大影响，从而避免了拜占庭鲁棒FL算法的检测。
- 这两种攻击方法的最终目标是提高全局模型的错误率并降低全局模型的可用性。

防御目标：

- 鲁棒性：将恶意客户对全局模型准确性和可用性的影响降至最低。
- 准确性：期望mean detection method (MDM)和多重检测机制能够检测FL中存在的所有恶意客户端。
- 效率：该方法不会给客户端带来额外的时间开销，并且消除了复杂的数学运算。

防御者的知识和能力：

- 假设恶意客户端的数量未知。假设服务器可以访问全局模型以及客户端上传的所有本地模型更新。
- 服务器需要收集一个小数据集（我们可以称之为后门数据集），以获得带水印的全局模型。服务器需要在收集的图像数据集中选择一些图像来嵌入后门触发器。它还设置了比例因子b，以表示包含带有触发器的图像的后门数据集的比例。虽然我们的解决方案需要一个数据集来支持它，但它不涉及客户端的私有数据集。

#### 4 机制

系统概览：

- 首先，WMDefense训练全局模型，基于收集的后门数据集（后门数据的比例为b=0.2）来嵌入一个水印。
- 然后，取出每个本地客户端的权重来更新重构模型，然后在b=1的后门数据集上执行分类测试，以获得不同客户端的本地模型在后门数据集的测试精度，即水印衰退的程度。
- 最后，我们使用均值检测方法（MDM）根据所有客户模型的水印衰退程度来区分恶意客户。
- 对于三次检测中水印衰退程度相对较大的客户，我们将其添加到恶意客户集。在全局模型聚合算法中，我们阻止恶意客户端集中所有客户端的本地模型更新。在所有三次恶意客户端检测之后，其余的良性客户端可以简单地按照正常的FL过程来训练全局模型。

WMDefense算法：四步迭代三次

- 在服务器端，服务器首先初始化全局模型。然后使用b=0.2的后门数据集训练全局模型。根据模型水印技术，我们现在有了包含水印的全局模型。
- 服务器将带有水印的全局模型分发给所有客户端。
- 客户端使用全局模型作为预训练模型，在私有数据集上训练其局部模型。使用正常模型训练方法训练局部模型。完成本地训练后，客户机将本地模型上传到服务器。
- 服务器使用b=1的后门数据集作为测试数据集，以获得用户上传的所有本地模型更新的测试精度ACC_i。之后，服务器通过MDM算法过滤出可能的恶意客户端。然后我们对它们的状态代码执行加一运算。第三轮检测后，我们比较所有客户端的状态代码，对于值大于或等于2的客户端，我们将其视为恶意客户端。最后，我们使用FedAvg聚合全局模型以进行良性客户端模型更新。

Mean Detection Method（MDM）

- 恶意客户端的检测主要基于其模型中水印衰退的程度。在MDM中，我们设置状态代码S来标记良性和恶意客户端。
- 我们将后门数据集上每个客户端的局部模型分类精度ACC_i与平均ACC进行比较，并将小于平均ACC的客户端定义为可能的恶意客户端。然后添加可能的恶意用户的状态代码，以指示我们的检测结果。该方法使用平均值作为分类界限，从而增强了后门数据集精度的差异性能。这种不同的性能足以让我们区分恶意客户端的干扰和其他良性的本地模型。

Multi-detection Mechanism

- 使用多检测机制来降低恶意客户端的错误检测率。
- 为每个客户端设置一个初始值为0的状态码。如果该客户端被分类为可能的恶意客户端，我们将对状态码的值执行加一操作。
- 在前三轮联邦学习中执行多检测机制。每个检测对被分类为可能的恶意客户端的客户端的状态代码执行加一操作。在第三轮检测之后，我们将状态代码大于或等于2的客户端分类为真正的恶意客户端，并将它们添加到恶意客户端集中。

#### 总结

WMDefense与现有拜占庭鲁棒FL算法的区别在于：通过嵌入模型水印的下降程度来检测拜占庭恶意客户端。

未来的研究方向：优化多重检测机制，并将算法改进为插件应用程序。此外，作者会将WMDefense应用于非IID联邦学习设置，以查看是否可以获得良好的结果。

### Vulnerabilities in Federated Learning

https://ieeexplore.ieee.org/document/9411833/

#### VII. Future Directions

可以探索的方向：

- 针对FL的攻击可以通过异常检测或鲁棒聚集算法来检测。但是不清楚在攻击FL模型时是否所有恶意参与者都有相同的威胁级别
- FL中的许多先前工作没有考虑神经网络的分层分解，而是将其视为没有内部结构的黑盒函数。在层级粒度上理解深度学习模型可能是掌握新攻击和防御的关键。
  - 特别是[133]中的观察和结论表明，网络层在泛化方面表现不同。
  - 作者建立了一个简单的实验，在该实验中，在简化的MNIST数据集上训练由10个相同层组成的11层MLP网络，这样，孤立地训练10个层中的任何一层都可以获得100%的训练精度。该实验的目的是证明使用单个隔离层进行训练将揭示其在网络中的重要性。
  - 第一层通过实现比网络最后一层更高的测试精度，说明了促进泛化的卓越能力。因此，层在泛化方面并不相等。上述观察引发了我们对一个重要问题的关注：未来的协同防御系统能否使用分裂学习[134]在各个层面上运作，而不是将整个模型视为一个黑匣子？
- FL基于多来源，不可能坚定地保证所有参与方都被认为是可信的。可以通过为每个参与者建立信任级别来消除许多攻击。确保和建立信任减少了对先进应对措施的需求，回到了标准的合作项目原则。
- 本文调查的工作范围表明，后门攻击仍然是FL安全的一个重大挑战。出于防御难以检测的攻击的重要性，我们提出了一个问题，即是否有一种方法可以通过智能模型设计保证抵御后门攻击的弹性。此外，学习数据分布和检查点模型可以作为未来FL系统的潜在防御机制。
- 由于隐私限制，对FL系统的攻击没有简单直接的防御措施。应探索适应性和主动防御技术，以保护FL免受对手的攻击。在规模更大的FL系统中，建立可执行的合作协议可能不切实际。一些恶意的客户端可能故意试图降低性能、关闭系统或从其他方提取信息。因此，需要安全策略来缓解这些风险，例如模型提交的高级加密、各方的安全认证、行动的可追溯性、差异隐私、验证系统、执行完整性、模型保密性以及针对可疑行动的保护。

